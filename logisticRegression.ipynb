{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIQUE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD OUR DATA SET\n",
    "##  as we have two dataset we have to caoncatenate its first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>urlDrugName</th>\n",
       "      <th>rating</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>sideEffects</th>\n",
       "      <th>condition</th>\n",
       "      <th>benefitsReview</th>\n",
       "      <th>sideEffectsReview</th>\n",
       "      <th>commentsReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1366</td>\n",
       "      <td>biaxin</td>\n",
       "      <td>9</td>\n",
       "      <td>Considerably Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>sinus infection</td>\n",
       "      <td>The antibiotic may have destroyed bacteria cau...</td>\n",
       "      <td>Some back pain, some nauseau.</td>\n",
       "      <td>Took the antibiotics for 14 days. Sinus infect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3724</td>\n",
       "      <td>lamictal</td>\n",
       "      <td>9</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>bipolar disorder</td>\n",
       "      <td>Lamictal stabilized my serious mood swings. On...</td>\n",
       "      <td>Drowsiness, a bit of mental numbness. If you t...</td>\n",
       "      <td>Severe mood swings between hypomania and depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3824</td>\n",
       "      <td>depakene</td>\n",
       "      <td>4</td>\n",
       "      <td>Moderately Effective</td>\n",
       "      <td>Severe Side Effects</td>\n",
       "      <td>bipolar disorder</td>\n",
       "      <td>Initial benefits were comparable to the brand ...</td>\n",
       "      <td>Depakene has a very thin coating, which caused...</td>\n",
       "      <td>Depakote was prescribed to me by a Kaiser psyc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>969</td>\n",
       "      <td>sarafem</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>No Side Effects</td>\n",
       "      <td>bi-polar / anxiety</td>\n",
       "      <td>It controlls my mood swings. It helps me think...</td>\n",
       "      <td>I didnt really notice any side effects.</td>\n",
       "      <td>This drug may not be for everyone but its wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>696</td>\n",
       "      <td>accutane</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>nodular acne</td>\n",
       "      <td>Within one week of treatment superficial acne ...</td>\n",
       "      <td>Side effects included moderate to severe dry s...</td>\n",
       "      <td>Drug was taken in gelatin tablet at 0.5 mg per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 urlDrugName  rating           effectiveness  \\\n",
       "0        1366      biaxin       9  Considerably Effective   \n",
       "1        3724    lamictal       9        Highly Effective   \n",
       "2        3824    depakene       4    Moderately Effective   \n",
       "3         969     sarafem      10        Highly Effective   \n",
       "4         696    accutane      10        Highly Effective   \n",
       "\n",
       "           sideEffects           condition  \\\n",
       "0    Mild Side Effects     sinus infection   \n",
       "1    Mild Side Effects    bipolar disorder   \n",
       "2  Severe Side Effects    bipolar disorder   \n",
       "3      No Side Effects  bi-polar / anxiety   \n",
       "4    Mild Side Effects        nodular acne   \n",
       "\n",
       "                                      benefitsReview  \\\n",
       "0  The antibiotic may have destroyed bacteria cau...   \n",
       "1  Lamictal stabilized my serious mood swings. On...   \n",
       "2  Initial benefits were comparable to the brand ...   \n",
       "3  It controlls my mood swings. It helps me think...   \n",
       "4  Within one week of treatment superficial acne ...   \n",
       "\n",
       "                                   sideEffectsReview  \\\n",
       "0                      Some back pain, some nauseau.   \n",
       "1  Drowsiness, a bit of mental numbness. If you t...   \n",
       "2  Depakene has a very thin coating, which caused...   \n",
       "3            I didnt really notice any side effects.   \n",
       "4  Side effects included moderate to severe dry s...   \n",
       "\n",
       "                                      commentsReview  \n",
       "0  Took the antibiotics for 14 days. Sinus infect...  \n",
       "1  Severe mood swings between hypomania and depre...  \n",
       "2  Depakote was prescribed to me by a Kaiser psyc...  \n",
       "3  This drug may not be for everyone but its wond...  \n",
       "4  Drug was taken in gelatin tablet at 0.5 mg per...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our dataset\n",
    "data1 = pd.read_table('drugLibTest_raw.tsv',sep='\\t')\n",
    "data2 = pd.read_table('drugLibTrain_raw.tsv',sep='\\t')\n",
    "data = pd.concat((data1,data2), axis=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then i will group the data according to the length, \n",
    "and for that I will use dummies to convert my strings values in\n",
    "numerical values for the data having a small length and countvectorize for the data having\n",
    "a long length for the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data[['effectiveness','sideEffects']]\n",
    "#data['benefitsReview','commentsReview']= data['benefitsReview','commentsReview'].str.lower()\n",
    "text= data[['benefitsReview','commentsReview']].astype(str).apply(lambda z: ' '.join(z),axis=1)\n",
    "text = text.str.lower()\n",
    "texte= text.replace('[\\d+]')#remove numbers\n",
    "d2 = texte\n",
    "data3 = pd.get_dummies(d1)\n",
    " #list of text documents\n",
    "text = d2\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer( binary = True ,min_df=50,\n",
    "                            stop_words ='english', max_features =50,\n",
    "                                                         )\n",
    "# tokenize and build vocab\n",
    "tati = vectorizer.fit_transform(text)\n",
    "data_text=tati.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " after that i transform it into dataframe and concatenate the both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4143 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   ...  40  41  42  43  44  45  46  \\\n",
       "0      1   0   0   0   0   0   1   0   0   0  ...   0   1   0   0   0   0   0   \n",
       "1      0   1   0   0   0   0   1   0   0   0  ...   1   0   0   0   1   0   0   \n",
       "2      0   0   0   0   1   0   0   0   0   1  ...   0   0   0   0   0   0   0   \n",
       "3      0   1   0   0   0   0   0   0   1   0  ...   0   0   0   0   0   0   0   \n",
       "4      0   1   0   0   0   0   1   0   0   0  ...   0   0   1   0   0   1   1   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "4138   0   1   0   0   0   0   1   0   0   0  ...   0   1   0   0   0   0   0   \n",
       "4139   0   0   1   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   \n",
       "4140   0   0   0   1   0   0   0   1   0   0  ...   0   0   0   0   0   0   0   \n",
       "4141   1   0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   1   0   \n",
       "4142   0   0   0   0   1   0   0   1   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "      47  48  49  \n",
       "0      0   0   0  \n",
       "1      0   0   0  \n",
       "2      1   0   1  \n",
       "3      0   0   0  \n",
       "4      0   0   0  \n",
       "...   ..  ..  ..  \n",
       "4138   0   0   0  \n",
       "4139   1   1   1  \n",
       "4140   0   0   0  \n",
       "4141   0   0   0  \n",
       "4142   0   0   0  \n",
       "\n",
       "[4143 rows x 60 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = pd.DataFrame(data_text)\n",
    "D = pd.concat([pd.DataFrame(data3.values),pd.DataFrame(data_text.values)],axis=1)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define my features and target. here i use the whole dataset ie 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rating'] =data['rating'].apply(lambda x  :x-1)\n",
    "y=data['rating'].values\n",
    "x = D.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogistiqueRegression(object):\n",
    "    \n",
    "    '''\n",
    "    the logistique regression algorithm\n",
    "    '''\n",
    "    #initialise our variables\n",
    "    def __init__(self,lr,n_iters,weight,x,y):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weight = weight\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "      \n",
    "    #the softmax function\n",
    "    def Softmax(self,z):\n",
    "        \n",
    "        res = np.exp(z.T) / np.sum(np.exp(z), axis=1).T\n",
    "        return res.T\n",
    "    \n",
    "    #cost function\n",
    "    def cost(self):\n",
    "        soft = self.Softmax(z)\n",
    "        cost = 0\n",
    "        for i in range(len(self.y)):\n",
    "            for k in range(len(classes)):\n",
    "                if self.y[i] == k:\n",
    "                    cost+= -np.log(soft[i,k])\n",
    "        return cost\n",
    "    \n",
    "    #the gradient\n",
    "    def gradient(self):\n",
    "        classes = np.unique(self.y)\n",
    "        soft = self.Softmax(z)\n",
    "        gradien = np.zeros((len(classes),self.x.shape[1]))\n",
    "        \n",
    "        for i in range(len(classes)):\n",
    "            sume=0\n",
    "            for k in range(self.x.shape[0]):\n",
    "                if y[k] == i:\n",
    "                    sume+=  x[i]*(1-soft[k,i]) \n",
    "                else:\n",
    "                    sume+= x[i]*(-soft[k,i])\n",
    "            gradien[i] = sume\n",
    "        return -gradien\n",
    "      \n",
    "        \n",
    "    def fit(self):\n",
    "        classes = np.unique(self.y)\n",
    "        grad = self.gradient()\n",
    "        for i in range(len(classes)):\n",
    "            for j in range(self.n_iters):\n",
    "                self.weight[:,i] = self.weight[:,i] - self.lr * grad[i]\n",
    "               # print(self.weight.shape)\n",
    "        return self.weight \n",
    "    \n",
    "    #the prediction\n",
    "    def prediction(self,t):\n",
    "        return np.argmax(self.Softmax(np.dot(t,self.fit())),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       ...,\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y)\n",
    "weight = np.zeros((x.shape[1], len(classes)))\n",
    "z=np.dot(x,weight)\n",
    "res = LogistiqueRegression(0.001,100,weight,x,y).Softmax(z)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8572.52430121726"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = LogistiqueRegression(0.001,100,weight,x,y).cost()\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 278.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  278.3,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  278.3,  278.3,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,  278.3,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. ,  219.3,   -0. ,   -0. ,   -0. ,   -0. ,  219.3,   -0. ,\n",
       "          -0. ,   -0. ,  219.3,   -0. ,   -0. ,   -0. ,  219.3,   -0. ,\n",
       "         219.3,   -0. ,   -0. ,  219.3,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,  219.3,   -0. ,  219.3,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,  219.3,   -0. ,  219.3,  219.3,   -0. ,   -0. ,   -0. ,\n",
       "         219.3,  219.3,  219.3,   -0. ,   -0. ,   -0. ,  219.3,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. ,   -0. ,   -0. ,   -0. ,  263.3,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,  263.3,   -0. ,  263.3,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         263.3,  263.3,  263.3,   -0. ,   -0. ,  263.3,   -0. ,  263.3,\n",
       "          -0. ,   -0. ,  263.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         263.3,   -0. ,  263.3,   -0. ,   -0. ,   -0. ,   -0. ,  263.3,\n",
       "          -0. ,  263.3,   -0. ,  263.3,   -0. ,  263.3,  263.3,   -0. ,\n",
       "         263.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,  263.3,   -0. ,  263.3],\n",
       "       [  -0. ,  188.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         188.3,   -0. ,   -0. ,   -0. ,  188.3,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  188.3,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  188.3,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         188.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. ,  199.3,   -0. ,   -0. ,   -0. ,   -0. ,  199.3,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  199.3,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  199.3,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,  199.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         199.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  199.3,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,  199.3,   -0. ,   -0. ,  199.3,\n",
       "         199.3,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. ,   -0. ,   -0. ,  -56.7,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         -56.7,   -0. ,   -0. ,   -0. ,  -56.7,   -0. ,  -56.7,   -0. ,\n",
       "          -0. ,  -56.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,  -56.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,  -56.7,   -0. ,  -56.7,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,  -56.7,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,  -56.7,   -0. ],\n",
       "       [-333.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. , -333.7,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. , -333.7, -333.7,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. , -198.7,   -0. ,   -0. ,   -0. ,   -0. , -198.7,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. , -198.7,   -0. ,   -0. ,   -0. ,\n",
       "        -198.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. , -198.7,\n",
       "          -0. ,   -0. , -198.7,   -0. ,   -0. , -198.7,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "        -198.7,   -0. ,   -0. ,   -0. ,   -0. , -198.7,   -0. ,   -0. ,\n",
       "        -198.7,   -0. ,   -0. ,   -0. ],\n",
       "       [  -0. , -553.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "        -553.7,   -0. , -553.7,   -0. ,   -0. ,   -0. , -553.7, -553.7,\n",
       "          -0. , -553.7,   -0. , -553.7,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "        -553.7,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. , -553.7,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. , -553.7,   -0. ,   -0. , -553.7, -553.7,   -0. ,   -0. ,\n",
       "          -0. ,   -0. , -553.7, -553.7,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. , -553.7, -553.7],\n",
       "       [  -0. ,   -0. ,   -0. ,  414.3,   -0. ,  414.3,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,  414.3,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "         414.3,   -0. ,  414.3,   -0. ,  414.3,   -0. ,   -0. ,  414.3,\n",
       "          -0. ,  414.3,  414.3,   -0. ,   -0. ,   -0. ,   -0. ,  414.3,\n",
       "          -0. ,  414.3,   -0. ,   -0. ,   -0. ,   -0. ,   -0. ,  414.3,\n",
       "          -0. ,  414.3,   -0. ,  414.3,   -0. ,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ,  414.3,   -0. ,   -0. ,   -0. ,\n",
       "          -0. ,   -0. ,   -0. ,   -0. ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = LogistiqueRegression(0.001,100,weight,x,y).gradient()\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x\n",
    "X_test = x\n",
    "Y_train = y\n",
    "Y_test = y\n",
    "d=LogistiqueRegression(0.001,100,weight,x,y).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, ..., 6, 8, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=LogistiqueRegression(0.001,100,weight,x,y).prediction(X_test)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.271542360608258"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(f.reshape(1,-1) == Y_test.reshape(1,-1))/len(Y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = int(len(x)* 10/100)\n",
    "x = x[:percentage,]\n",
    "y = y[:percentage]\n",
    "X_train = x[20:]\n",
    "X_test = x[:20]\n",
    "Y_train = y[20:]\n",
    "Y_test = y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = LogistiqueRegression(0.001,100,weight,x,y).Softmax(z)\n",
    "cost = LogistiqueRegression(0.001,100,weight,x,y).cost()\n",
    "gradient = LogistiqueRegression(0.001,100,weight,x,y).gradient()\n",
    "d=LogistiqueRegression(0.001,100,weight,x,y).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8, 8, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=LogistiqueRegression(0.001,100,weight,x,y).prediction(X_test)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(f.reshape(1,-1) == Y_test.reshape(1,-1))/len(Y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 30% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = int(len(x)* 30/100)\n",
    "x = x[:percentage,]\n",
    "y = y[:percentage]\n",
    "X_train = x[20:]\n",
    "X_test = x[:20]\n",
    "Y_train = y[20:]\n",
    "Y_test = y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = LogistiqueRegression(0.001,100,weight,x,y).Softmax(z)\n",
    "cost = LogistiqueRegression(0.001,100,weight,x,y).cost()\n",
    "gradient = LogistiqueRegression(0.001,100,weight,x,y).gradient()\n",
    "d=LogistiqueRegression(0.001,100,weight,x,y).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8, 8, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=LogistiqueRegression(0.001,100,weight,x,y).prediction(X_test)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(f.reshape(1,-1) == Y_test.reshape(1,-1))/len(Y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 60% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = int(len(x)* 60/100)\n",
    "x = x[:percentage,]\n",
    "y = y[:percentage]\n",
    "X_train = x[20:]\n",
    "X_test = x[:20]\n",
    "Y_train = y[20:]\n",
    "Y_test = y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = LogistiqueRegression(0.001,100,weight,x,y).Softmax(z)\n",
    "cost = LogistiqueRegression(0.001,100,weight,x,y).cost()\n",
    "gradient = LogistiqueRegression(0.001,100,weight,x,y).gradient()\n",
    "d=LogistiqueRegression(0.001,100,weight,x,y).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8, 8, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=LogistiqueRegression(0.001,100,weight,x,y).prediction(X_test)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(f.reshape(1,-1) == Y_test.reshape(1,-1))/len(Y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we note that based on the accuracy and the prediction, using all the data we we do not have a better perfomance of the model.So performance increases when data are reduced. We can conclude that the Logistique regression model is appropriate for this type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
